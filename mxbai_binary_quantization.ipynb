{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary embeddings with [mixedbread-ai/mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n",
    "\n",
    "Our model was trained to have a non-'clunky' embeddings space. This allows for quantizing the embeddings with low performance loss compared to techniques like Matryoshka. With binary embeddings, we can use the Hamming distance, which is well optimized for CPUs.\n",
    "\n",
    "In general, the approach is divided into 2 steps:\n",
    "\n",
    "1. Retrieve candidates based on Hamming distance.\n",
    "2. Rescore the candidates based on the dot product between the binary embedding and the floating embedding of the query.\n",
    "\n",
    "We find that we can retain ~96-99% of the performance, achieve ~40x faster retrieval, and realize 32x storage savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers datasets beir faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the worlds best model xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrecCovid is a nice benchmark, not too large, not too small, also pretty difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mteb/trec-covid\"\n",
    "dataset = load_dataset(task, \"corpus\")\n",
    "docs_ids = dataset[\"corpus\"][\"_id\"]\n",
    "features = [d[\"title\"] + \" \" + d[\"text\"] for d in dataset[\"corpus\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's speedup the calculation by using fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.half()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On 4xA100 it should take ~2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings computed. Shape: (171332, 1024)\n"
     ]
    }
   ],
   "source": [
    "pool = model.start_multi_process_pool()\n",
    "\n",
    "# normalize_embeddings=True will normalize the embeddings to unit length before indexing so the dot product is equal to the cosine similarity\n",
    "emb = model.encode_multi_process(features, pool, normalize_embeddings=True)\n",
    "print(\"Embeddings computed. Shape:\", emb.shape)\n",
    "\n",
    "model.stop_multi_process_pool(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP32 Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(emb.shape[1])\n",
    "index.add(emb)\n",
    "faiss.write_index(index, \"index_fp32.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Index, convert embeddings using simple thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary embeddings computed. Shape: (171332, 128)\n"
     ]
    }
   ],
   "source": [
    "bemb = np.where(emb < 0, 0, 1).astype(np.uint8)\n",
    "bemb = np.packbits(bemb).reshape(bemb.shape[0], -1)\n",
    "print(\"Binary embeddings computed. Shape:\", bemb.shape)\n",
    "num_dim = emb.shape[1]\n",
    "bindex = faiss.IndexBinaryFlat(num_dim)\n",
    "bindex.add(bemb)\n",
    "faiss.write_index_binary(bindex, \"index_binary.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compression size is ~32 as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size of index_fp32.faiss: 701775917\n",
      "File size of index_binary.faiss: 21930529\n",
      "Compression ratio: 31.999953899880847\n"
     ]
    }
   ],
   "source": [
    "# check file size\n",
    "fp32_index_size = os.path.getsize(\"index_fp32.faiss\")\n",
    "binary_index_size = os.path.getsize(\"index_binary.faiss\")\n",
    "print(\"File size of index_fp32.faiss:\", fp32_index_size)\n",
    "print(\"File size of index_binary.faiss:\", binary_index_size)\n",
    "print(\"Compression ratio:\", fp32_index_size / binary_index_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some BEIR stuff for the eval later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels_df = load_dataset(task)[\"test\"]\n",
    "qrels = {}\n",
    "for row in qrels_df:\n",
    "    qid = row['query-id']\n",
    "    cid = row['corpus-id']\n",
    "    \n",
    "    if row['score'] > 0:\n",
    "        if qid not in qrels:\n",
    "            qrels[qid] = {}\n",
    "        qrels[qid][cid] = int(row['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = load_dataset(task, \"queries\")\n",
    "queries = queries.filter(lambda x: x['_id'] in qrels)\n",
    "\n",
    "query_ids = queries[\"queries\"][\"_id\"]\n",
    "queries = [\"Represent this sentence for searching relevant passages: \" + d[\"text\"] for d in queries[\"queries\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.float()\n",
    "query_emb = model.encode(queries, convert_to_numpy=True, normalize_embeddings=True)\n",
    "query_bemb = np.where(query_emb < 0, 0, 1).astype(np.uint8) # binarize\n",
    "query_bemb = np.packbits(query_bemb).reshape(query_bemb.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_search(index, queries_emb, k=[10, 100], float_embed = None):\n",
    "    start_time = time.time()\n",
    "    faiss_scores, faiss_doc_ids = index.search(queries_emb, max(k))\n",
    "    print(f\"Search took {(time.time()-start_time):.4f} sec\")\n",
    "    \n",
    "    query2id = {idx: qid for idx, qid in enumerate(query_ids)}\n",
    "    doc2id = {idx: cid for idx, cid in enumerate(docs_ids)}\n",
    "    id2doc = {cid: idx for idx, cid in enumerate(docs_ids)}\n",
    "\n",
    "    faiss_results = {}\n",
    "    for idx in range(0, len(faiss_scores)):\n",
    "        qid = query2id[idx]\n",
    "        doc_scores = {doc2id[doc_id]: score.item() for doc_id, score in zip(faiss_doc_ids[idx], faiss_scores[idx])}\n",
    "       \n",
    "        # Rescore\n",
    "        if float_embed is not None:\n",
    "            bin_doc_emb = np.asarray([index.reconstruct(id2doc[doc_id]) for doc_id in doc_scores])\n",
    "            bin_doc_emb_unpacked = np.unpackbits(bin_doc_emb, axis=-1).astype(\"int\")\n",
    "            \n",
    "            scores_cont = (float_embed[idx] @ bin_doc_emb_unpacked.T)\n",
    "            doc_scores = {doc_id: score_cont for doc_id, score_cont in zip(doc_scores, scores_cont)}\n",
    "\n",
    "        faiss_results[qid] = doc_scores\n",
    "\n",
    "        \n",
    "    ndcg, map_score, recall, precision = EvaluateRetrieval.evaluate(qrels, faiss_results, k)\n",
    "    acc = EvaluateRetrieval.evaluate_custom(qrels, faiss_results, [3, 5, 10], metric=\"acc\")\n",
    "    print(ndcg)\n",
    "    print(recall)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Normal exact search\n",
    "We mostly care about NDCG@10 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search took 0.4663 sec\n",
      "{'NDCG@10': 0.75558, 'NDCG@100': 0.56317}\n",
      "{'Recall@10': 0.02136, 'Recall@100': 0.13842}\n",
      "{'Accuracy@3': 0.98, 'Accuracy@5': 1.0, 'Accuracy@10': 1.0}\n"
     ]
    }
   ],
   "source": [
    "faiss_search(index, query_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W/O Rescoring\n",
    "\n",
    "We loose around 53% of the performance. But its pretty fast ~30-40x faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search took 0.0094 sec\n",
      "{'NDCG@10': 0.35133, 'NDCG@100': 0.43985}\n",
      "{'Recall@10': 0.00866, 'Recall@100': 0.1221}\n",
      "{'Accuracy@3': 0.66, 'Accuracy@5': 0.76, 'Accuracy@10': 0.84}\n"
     ]
    }
   ],
   "source": [
    "faiss_search(bindex, query_bemb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Rescoring\n",
    "\n",
    "Still extremely fast, with the difference that we retain 99% of the performance. We verified similar behavior for SciFact and ArguAna. Accuracy was also boosted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search took 0.0153 sec\n",
      "{'NDCG@10': 0.75496, 'NDCG@100': 0.51706}\n",
      "{'Recall@10': 0.02128, 'Recall@100': 0.1221}\n",
      "{'Accuracy@3': 1.0, 'Accuracy@5': 1.0, 'Accuracy@10': 1.0}\n"
     ]
    }
   ],
   "source": [
    "faiss_search(bindex, query_bemb, float_embed=query_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Binary embedding enables extremely fast retrieval and low storage usage, at the expense of a slight performance loss, which can be mitigated by using a reranker. This has cool applications for on-device usage, large-scale developments, etc. We should also explore its potential for other tasks, such as clustering and deduplication at scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
